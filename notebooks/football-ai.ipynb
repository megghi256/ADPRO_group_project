{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFS_cSAqNKHi"
      },
      "source": [
        "# Football AI\n",
        "\n",
        "---\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/roboflow/sports)\n",
        "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/camera-calibration-sports-computer-vision/)\n",
        "[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=aBVGKoNZQUw)\n",
        "\n",
        "Let's build a Football AI system to dig deeper into match stats! We'll use computer vision and machine learning to track players, determine which team is which, and even calculate stuff like ball possession and speed. This tutorial is perfect if you want to get hands-on with sports analytics and see how AI can take your football analysis to the next level.\n",
        "\n",
        "![football AI diagram](https://media.roboflow.com/notebooks/examples/football-ai-diagram.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qXFCSup2T0L"
      },
      "source": [
        "## Before you start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0NnZCmC2Kmx"
      },
      "source": [
        "### Configure your API keys\n",
        "\n",
        "- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate new token.\n",
        "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.\n",
        "- In Colab, go to the left pane and click on `Secrets` (ðŸ”‘).\n",
        "    - Store HuggingFace Access Token under the name `HF_TOKEN`.\n",
        "    - Store Roboflow API Key under the name `ROBOFLOW_API_KEY`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "os.environ[\"ROBOFLOW_API_KEY\"] = userdata.get(\"ROBOFLOW_API_KEY\")"
      ],
      "metadata": {
        "id": "WQX_3KmAczkW",
        "outputId": "ed42ec20-d06e-4e6d-eb21-b4fb9cb4d74e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret HF_TOKEN does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-defa0cab2495>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HF_TOKEN\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HF_TOKEN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ROBOFLOW_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ROBOFLOW_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret HF_TOKEN does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og_H-sidUcaX"
      },
      "source": [
        "### Select the runtime\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8qxMYQZmXgz",
        "outputId": "97d2acbd-f8dd-4e36-ffab-dac0c65807a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May 12 15:04:36 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   41C    P8             12W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZXYNzU4tH4Q"
      },
      "source": [
        "## Install dependencjes\n",
        "\n",
        "**Note:** Let's install the `inference-gpu` library, which will be used to efficiently run our object detection and keypoint detection models on GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgZIoz0YTvV-",
        "outputId": "9bfe48bd-d84b-4fdc-d2e0-fb20314220be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/105.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.4/99.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m280.8/280.8 MB\u001b[0m \u001b[31m202.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q gdown inference-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrE633rgXSZK"
      },
      "source": [
        "**Note:** Let's install the sports repository directly from GitHub. The sports repository contains a variety of football-related utilities that we'll use along the way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcuIp4oXBClh"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/roboflow/sports.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWAljqbWYJPX"
      },
      "source": [
        "**Note:** Let's make sure we have the latest features in the supervision library by installing version `0.23.0` or higher."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep supervision"
      ],
      "metadata": {
        "id": "TOk2rwjbckZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WKBNI87YrEt"
      },
      "source": [
        "**Note:** Let's download a few sample videos from the [DFL - Bundesliga Data Shootout](https://www.kaggle.com/competitions/dfl-bundesliga-data-shootout) Kaggle competition using gdown.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gheSLBSMnTIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Rq5-SX9WfsY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"ONNXRUNTIME_EXECUTION_PROVIDERS\"] = \"[CUDAExecutionProvider]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "dUkCAKNSuG_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVlyqi7pVFDo"
      },
      "source": [
        "## ball, player, goalkeeper and referee detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV9voyVS6Aqi"
      },
      "outputs": [],
      "source": [
        "from inference import get_model\n",
        "from google.colab import userdata\n",
        "from ultralytics import YOLO\n",
        "\n",
        "PLAYER_DETECTION_MODEL = YOLO(\"/content/drive/MyDrive/TestTesi/mine /output field/best-player-tracking.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zz-hd4mdZ4UD"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/TestTesi/Ama_test1.mp4\"\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "sv.plot_image(frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duGEOk-j4n4i"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/TestTesi/Ama_test1.mp4\"\n",
        "\n",
        "box_annotator = sv.BoxAnnotator(\n",
        "    color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),\n",
        "    thickness=2\n",
        ")\n",
        "label_annotator = sv.LabelAnnotator(\n",
        "    color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),\n",
        "    text_color=sv.Color.from_hex('#000000')\n",
        ")\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=5)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.predict(frame, conf=0.28)[0]\n",
        "detections = sv.Detections.from_ultralytics(result)\n",
        "\n",
        "labels = [\n",
        "    f\"{class_name} {confidence:.2f}\"\n",
        "    for class_name, confidence\n",
        "    in zip(detections['class_name'], detections.confidence)\n",
        "]\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = box_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=detections)\n",
        "annotated_frame = label_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=detections,\n",
        "    labels=labels)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-4F9NB48U1_"
      },
      "source": [
        "## video game style visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWoDdvikt4TR"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/TestTesi/Ama_test1.mp4\"\n",
        "BALL_ID = 0\n",
        "\n",
        "ellipse_annotator = sv.EllipseAnnotator(\n",
        "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
        "    thickness=2\n",
        ")\n",
        "triangle_annotator = sv.TriangleAnnotator(\n",
        "    color=sv.Color.from_hex('#FFD700'),\n",
        "    base=25,\n",
        "    height=21,\n",
        "    outline_thickness=1\n",
        ")\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=5)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.predict(frame, conf=0.18)[0]\n",
        "detections = sv.Detections.from_ultralytics(result)\n",
        "\n",
        "ball_detections = detections[detections.class_id == BALL_ID]\n",
        "ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
        "\n",
        "all_detections = detections[detections.class_id != BALL_ID]\n",
        "all_detections = all_detections.with_nms(threshold=0.15, class_agnostic=True)\n",
        "all_detections.class_id -= 1\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = ellipse_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=all_detections)\n",
        "annotated_frame = triangle_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=ball_detections)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNRVUGG4nz_J"
      },
      "source": [
        "## player tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ixZlM06Gmae"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/TestTesi/Ama_test1.mp4\"\n",
        "BALL_ID = 0\n",
        "\n",
        "ellipse_annotator = sv.EllipseAnnotator(\n",
        "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
        "    thickness=2\n",
        ")\n",
        "label_annotator = sv.LabelAnnotator(\n",
        "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
        "    text_color=sv.Color.from_hex('#000000'),\n",
        "    text_position=sv.Position.BOTTOM_CENTER\n",
        ")\n",
        "triangle_annotator = sv.TriangleAnnotator(\n",
        "    color=sv.Color.from_hex('#FFD700'),\n",
        "    base=25,\n",
        "    height=21,\n",
        "    outline_thickness=1\n",
        ")\n",
        "\n",
        "tracker = sv.ByteTrack()\n",
        "tracker.reset()\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=5)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.predict(frame, conf=0.18)[0]\n",
        "detections = sv.Detections.from_ultralytics(result)\n",
        "\n",
        "ball_detections = detections[detections.class_id == BALL_ID]\n",
        "ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
        "\n",
        "all_detections = detections[detections.class_id != BALL_ID]\n",
        "all_detections = all_detections.with_nms(threshold=0.15, class_agnostic=True)\n",
        "all_detections.class_id -= 1\n",
        "all_detections = tracker.update_with_detections(detections=all_detections)\n",
        "\n",
        "labels = [\n",
        "    f\"#{tracker_id}\"\n",
        "    for tracker_id\n",
        "    in all_detections.tracker_id\n",
        "]\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = ellipse_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=all_detections)\n",
        "annotated_frame = label_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=all_detections,\n",
        "    labels=labels)\n",
        "annotated_frame = triangle_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=ball_detections)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1smkPKfYm00"
      },
      "source": [
        "## split players into teams\n",
        "\n",
        "![football AI diagram](https://media.roboflow.com/notebooks/examples/football-ai-team-clustering.png)\n",
        "\n",
        "**Note:** Before training our player clustering model, we need to gather training data. To do this, we'll sample one frame per second, detect players within those frames, and then crop them out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr2smM9fMTSO"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/TestTesi/Ama_test1.mp4\"\n",
        "PLAYER_ID = 2\n",
        "STRIDE = 30\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(\n",
        "    source_path=SOURCE_VIDEO_PATH, start=5,stride=STRIDE)\n",
        "\n",
        "crops = []\n",
        "for frame in tqdm(frame_generator, desc='collecting crops'):\n",
        "    result = PLAYER_DETECTION_MODEL.predict(frame, conf=0.18)[0]\n",
        "    detections = sv.Detections.from_ultralytics(result)\n",
        "    detections = detections.with_nms(threshold=0.15, class_agnostic=True)\n",
        "    detections = detections[detections.class_id == PLAYER_ID]\n",
        "    players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]\n",
        "    crops += players_crops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmUMuTiq4_pT"
      },
      "source": [
        "**Note:** Here's a sample (100 elements) of the crops we've gathered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qImyErnMNhfC"
      },
      "outputs": [],
      "source": [
        "sv.plot_images_grid(crops[:100], grid_size=(10, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8COwN7TweZpK"
      },
      "source": [
        "**Note:** Next, we'll run [SigLIP](https://huggingface.co/docs/transformers/en/model_doc/siglip) to calculate embeddings for each of the crops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLcvVFrbOey8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, SiglipVisionModel\n",
        "\n",
        "SIGLIP_MODEL_PATH = 'google/siglip-base-patch16-224'\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "EMBEDDINGS_MODEL = SiglipVisionModel.from_pretrained(SIGLIP_MODEL_PATH).to(DEVICE)\n",
        "EMBEDDINGS_PROCESSOR = AutoProcessor.from_pretrained(SIGLIP_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrn67xnEQZzM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from more_itertools import chunked\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "crops = [sv.cv2_to_pillow(crop) for crop in crops]\n",
        "batches = chunked(crops, BATCH_SIZE)\n",
        "data = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(batches, desc='embedding extraction'):\n",
        "        inputs = EMBEDDINGS_PROCESSOR(images=batch, return_tensors=\"pt\").to(DEVICE)\n",
        "        outputs = EMBEDDINGS_MODEL(**inputs)\n",
        "        embeddings = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()\n",
        "        data.append(embeddings)\n",
        "\n",
        "data = np.concatenate(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1QUD1X25faA"
      },
      "source": [
        "**Note:** Using [UMAP](https://github.com/lmcinnes/umap), we project our embeddings from `(N, 768)` to `(N, 3)` and then perform a two-cluster division using [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EXV02O3SWsX"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "REDUCER = umap.UMAP(n_components=3)\n",
        "CLUSTERING_MODEL = KMeans(n_clusters=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfrRj4_PS1HD"
      },
      "outputs": [],
      "source": [
        "projections = REDUCER.fit_transform(data)\n",
        "clusters = CLUSTERING_MODEL.fit_predict(projections)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_gqJjBu6IRU"
      },
      "source": [
        "**Note:** Here's an interactive visualization of our results. Click on a dot to display its associated crop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXWLoCBtfPDH"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from typing import Dict, List\n",
        "from IPython.core.display import display, HTML\n",
        "from PIL import Image\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "def pil_image_to_data_uri(image: Image.Image) -> str:\n",
        "    buffered = BytesIO()\n",
        "    image.save(buffered, format=\"PNG\")\n",
        "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "    return f\"data:image/png;base64,{img_str}\"\n",
        "\n",
        "\n",
        "def display_projections(\n",
        "    labels: np.ndarray,\n",
        "    projections: np.ndarray,\n",
        "    images: List[Image.Image],\n",
        "    show_legend: bool = False,\n",
        "    show_markers_with_text: bool = True\n",
        ") -> None:\n",
        "    image_data_uris = {f\"image_{i}\": pil_image_to_data_uri(image) for i, image in enumerate(images)}\n",
        "    image_ids = np.array([f\"image_{i}\" for i in range(len(images))])\n",
        "\n",
        "    unique_labels = np.unique(labels)\n",
        "    traces = []\n",
        "    for unique_label in unique_labels:\n",
        "        mask = labels == unique_label\n",
        "        customdata_masked = image_ids[mask]\n",
        "        trace = go.Scatter3d(\n",
        "            x=projections[mask][:, 0],\n",
        "            y=projections[mask][:, 1],\n",
        "            z=projections[mask][:, 2],\n",
        "            mode='markers+text' if show_markers_with_text else 'markers',\n",
        "            text=labels[mask],\n",
        "            customdata=customdata_masked,\n",
        "            name=str(unique_label),\n",
        "            marker=dict(size=8),\n",
        "            hovertemplate=\"<b>class: %{text}</b><br>image ID: %{customdata}<extra></extra>\"\n",
        "        )\n",
        "        traces.append(trace)\n",
        "\n",
        "    # Calculate shared range for cube appearance\n",
        "    all_axes = projections\n",
        "    min_val = np.min(all_axes)\n",
        "    max_val = np.max(all_axes)\n",
        "    padding = (max_val - min_val) * 0.05\n",
        "    axis_range = [min_val - padding, max_val + padding]\n",
        "\n",
        "    fig = go.Figure(data=traces)\n",
        "    fig.update_layout(\n",
        "        scene=dict(\n",
        "            xaxis=dict(title='X', range=axis_range),\n",
        "            yaxis=dict(title='Y', range=axis_range),\n",
        "            zaxis=dict(title='Z', range=axis_range),\n",
        "            aspectmode='cube'  # Ensures equal scaling\n",
        "        ),\n",
        "        width=1000,\n",
        "        height=1000,\n",
        "        showlegend=show_legend\n",
        "    )\n",
        "\n",
        "    plotly_div = fig.to_html(full_html=False, include_plotlyjs=False, div_id=\"scatter-plot-3d\")\n",
        "\n",
        "    javascript_code = f\"\"\"\n",
        "    <script>\n",
        "        function displayImage(imageId) {{\n",
        "            var imageElement = document.getElementById('image-display');\n",
        "            var placeholderText = document.getElementById('placeholder-text');\n",
        "            var imageDataURIs = {image_data_uris};\n",
        "            imageElement.src = imageDataURIs[imageId];\n",
        "            imageElement.style.display = 'block';\n",
        "            placeholderText.style.display = 'none';\n",
        "        }}\n",
        "\n",
        "        var chartElement = document.getElementById('scatter-plot-3d');\n",
        "\n",
        "        chartElement.on('plotly_click', function(data) {{\n",
        "            var customdata = data.points[0].customdata;\n",
        "            displayImage(customdata);\n",
        "        }});\n",
        "    </script>\n",
        "    \"\"\"\n",
        "\n",
        "    html_template = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "        <head>\n",
        "            <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
        "            <style>\n",
        "                #image-container {{\n",
        "                    position: fixed;\n",
        "                    top: 0;\n",
        "                    left: 0;\n",
        "                    width: 200px;\n",
        "                    height: 200px;\n",
        "                    padding: 5px;\n",
        "                    border: 1px solid #ccc;\n",
        "                    background-color: white;\n",
        "                    z-index: 1000;\n",
        "                    box-sizing: border-box;\n",
        "                    display: flex;\n",
        "                    align-items: center;\n",
        "                    justify-content: center;\n",
        "                    text-align: center;\n",
        "                }}\n",
        "                #image-display {{\n",
        "                    width: 100%;\n",
        "                    height: 100%;\n",
        "                    object-fit: contain;\n",
        "                }}\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            {plotly_div}\n",
        "            <div id=\"image-container\">\n",
        "                <img id=\"image-display\" src=\"\" alt=\"Selected image\" style=\"display: none;\" />\n",
        "                <p id=\"placeholder-text\">Click on a data entry to display an image</p>\n",
        "            </div>\n",
        "            {javascript_code}\n",
        "        </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    display(HTML(html_template))\n",
        "\n",
        "display_projections(clusters, projections, crops)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka1H-BQk8LAq"
      },
      "source": [
        "**Note:** To simplify the use of the SigLIP, UMAP, and KMeans combo, I've packaged all these models into a [`TeamClassifier`](https://github.com/roboflow/sports/blob/06053616f1f8a8ae1fa936eb00dcdc2e4f888bb1/sports/common/team.py#L41) that you can find in the [sports](https://github.com/roboflow/sports) repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZG1DorZ8lSQ"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "from tqdm import tqdm\n",
        "from sports.common.team import TeamClassifier\n",
        "\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/TestTesi/Ama_test1.mp4\"\n",
        "PLAYER_ID = 2\n",
        "STRIDE = 30\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(\n",
        "    source_path=SOURCE_VIDEO_PATH, start=5, stride=STRIDE)\n",
        "\n",
        "crops = []\n",
        "for frame in tqdm(frame_generator, desc='collecting crops'):\n",
        "    result = PLAYER_DETECTION_MODEL.predict(frame, conf=0.18)[0]\n",
        "    detections = sv.Detections.from_ultralytics(result)\n",
        "    players_detections = detections[detections.class_id == PLAYER_ID]\n",
        "    players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]\n",
        "    crops += players_crops\n",
        "\n",
        "team_classifier = TeamClassifier(device=\"cuda\")\n",
        "team_classifier.fit(crops)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlHrjbIf-Yrk"
      },
      "source": [
        "**Note:** Time to assign goalkeepers to teams. We'll use a simple heuristic: calculate the average position (centroid) of the players belonging to both teams and then assign the goalkeeper to the team whose average position is closer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx_4g2DGC1Bd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "def resolve_goalkeepers_team_id(\n",
        "    players: sv.Detections,\n",
        "    goalkeepers: sv.Detections\n",
        ") -> np.ndarray:\n",
        "    goalkeepers_xy = goalkeepers.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
        "    players_xy = players.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
        "    team_0_centroid = players_xy[players.class_id == 0].mean(axis=0)\n",
        "    team_1_centroid = players_xy[players.class_id == 1].mean(axis=0)\n",
        "    goalkeepers_team_id = []\n",
        "    for goalkeeper_xy in goalkeepers_xy:\n",
        "        dist_0 = np.linalg.norm(goalkeeper_xy - team_0_centroid)\n",
        "        dist_1 = np.linalg.norm(goalkeeper_xy - team_1_centroid)\n",
        "        goalkeepers_team_id.append(0 if dist_0 < dist_1 else 1)\n",
        "\n",
        "    return np.array(goalkeepers_team_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwmQrEOHAPyi"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/TestTesi/Ama_test1.mp4\"\n",
        "BALL_ID = 0\n",
        "GOALKEEPER_ID = 1\n",
        "PLAYER_ID = 2\n",
        "REFEREE_ID = 3\n",
        "\n",
        "ellipse_annotator = sv.EllipseAnnotator(\n",
        "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
        "    thickness=2\n",
        ")\n",
        "label_annotator = sv.LabelAnnotator(\n",
        "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
        "    text_color=sv.Color.from_hex('#000000'),\n",
        "    text_position=sv.Position.BOTTOM_CENTER\n",
        ")\n",
        "triangle_annotator = sv.TriangleAnnotator(\n",
        "    color=sv.Color.from_hex('#FFD700'),\n",
        "    base=25,\n",
        "    height=21,\n",
        "    outline_thickness=1\n",
        ")\n",
        "\n",
        "tracker = sv.ByteTrack()\n",
        "tracker.reset()\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.predict(frame, conf=0.18)[0]\n",
        "detections = sv.Detections.from_ultralytics(result)\n",
        "\n",
        "ball_detections = detections[detections.class_id == BALL_ID]\n",
        "ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
        "\n",
        "all_detections = detections[detections.class_id != BALL_ID]\n",
        "all_detections = all_detections.with_nms(threshold=0.15, class_agnostic=True)\n",
        "all_detections = tracker.update_with_detections(detections=all_detections)\n",
        "\n",
        "goalkeepers_detections = all_detections[all_detections.class_id == GOALKEEPER_ID]\n",
        "players_detections = all_detections[all_detections.class_id == PLAYER_ID]\n",
        "referees_detections = all_detections[all_detections.class_id == REFEREE_ID]\n",
        "\n",
        "players_crops = [sv.crop_image(frame, xyxy) for xyxy in players_detections.xyxy]\n",
        "players_detections.class_id = team_classifier.predict(players_crops)\n",
        "\n",
        "goalkeepers_detections.class_id = resolve_goalkeepers_team_id(\n",
        "    players_detections, goalkeepers_detections)\n",
        "\n",
        "referees_detections.class_id -= 1\n",
        "\n",
        "all_detections = sv.Detections.merge([\n",
        "    players_detections, goalkeepers_detections, referees_detections])\n",
        "\n",
        "labels = [\n",
        "    f\"#{tracker_id}\"\n",
        "    for tracker_id\n",
        "    in all_detections.tracker_id\n",
        "]\n",
        "\n",
        "all_detections.class_id = all_detections.class_id.astype(int)\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = ellipse_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=all_detections)\n",
        "annotated_frame = label_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=all_detections,\n",
        "    labels=labels)\n",
        "annotated_frame = triangle_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=ball_detections)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4F_9d1YHKuj"
      },
      "source": [
        "## pitch keypoint detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts9f1goyCPEa"
      },
      "outputs": [],
      "source": [
        "from inference import get_model\n",
        "from google.colab import userdata\n",
        "\n",
        "ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')\n",
        "FIELD_DETECTION_MODEL_ID = \"football-field-detection-f07vi/14\"\n",
        "FIELD_DETECTION_MODEL = get_model(model_id=FIELD_DETECTION_MODEL_ID, api_key=ROBOFLOW_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "cjT6kqD2qTte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhiFgOGkUky5"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/TestTesi/Ama_test2.mp4\"\n",
        "\n",
        "vertex_annotator = sv.VertexAnnotator(\n",
        "    color=sv.Color.from_hex('#FF1493'),\n",
        "    radius=8)\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=5)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.13)[0]\n",
        "key_points = sv.KeyPoints.from_inference(result)\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = vertex_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    key_points=key_points)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HAVEmruAGpQ"
      },
      "source": [
        "**Note:** Notice that some of the keypoints we detected are in incorrect locations. These are keypoints with a low confidence level. Let's filter out these keypoints and keep only the ones the model is confident about."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWhRsjoZGRUa"
      },
      "source": [
        "## filter low confidence keypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4XW4_bDGjlF"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "import numpy as np\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/TestTesi/Ama_test1.mp4\"\n",
        "\n",
        "vertex_annotator = sv.VertexAnnotator(\n",
        "    color=sv.Color.from_hex('#FF1493'),\n",
        "    radius=8)\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=5)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "key_points = sv.KeyPoints.from_inference(result)\n",
        "\n",
        "filter = key_points.confidence[0] > 0.5\n",
        "frame_reference_points = key_points.xy[0][filter]\n",
        "frame_reference_key_points = sv.KeyPoints(\n",
        "    xy=frame_reference_points[np.newaxis, ...])\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = vertex_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    key_points=frame_reference_key_points)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7SUQtqbZ5QJ"
      },
      "source": [
        "## project pitch lines on frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojlmrERnBAIN"
      },
      "source": [
        "**Note:** The [sports](https://github.com/roboflow/sports) repository contains a [`SoccerPitchConfiguration`](https://github.com/roboflow/sports/blob/06053616f1f8a8ae1fa936eb00dcdc2e4f888bb1/sports/configs/soccer.py#L6) that provides information about the real-world geometry of the soccer pitch. It also includes utilities for visualizing elements located on the pitch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5y4tMWbaiC6"
      },
      "outputs": [],
      "source": [
        "from sports.annotators.soccer import draw_pitch\n",
        "from sports.configs.soccer import SoccerPitchConfiguration\n",
        "\n",
        "CONFIG = SoccerPitchConfiguration()\n",
        "\n",
        "annotated_frame = draw_pitch(CONFIG)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1abHrITPB3Bd"
      },
      "source": [
        "**Note:** It's time to utilize the keypoint pairs located on the camera perspective plane and the football pitch plane. The [sports](https://github.com/roboflow/sports) repository includes a [`ViewTransformer`](https://github.com/roboflow/sports/blob/06053616f1f8a8ae1fa936eb00dcdc2e4f888bb1/sports/common/view.py#L7), which employs homography for perspective transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjoG33jod7M8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import supervision as sv\n",
        "from sports.common.view import ViewTransformer\n",
        "\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/TestTesi/Ama_test1.mp4\"\n",
        "\n",
        "edge_annotator = sv.EdgeAnnotator(\n",
        "    color=sv.Color.from_hex('#00BFFF'),\n",
        "    thickness=2, edges=CONFIG.edges)\n",
        "vertex_annotator = sv.VertexAnnotator(\n",
        "    color=sv.Color.from_hex('#FF1493'),\n",
        "    radius=8)\n",
        "vertex_annotator_2 = sv.VertexAnnotator(\n",
        "    color=sv.Color.from_hex('#00BFFF'),\n",
        "    radius=8)\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=5)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "key_points = sv.KeyPoints.from_inference(result)\n",
        "\n",
        "filter = key_points.confidence[0] > 0.5\n",
        "frame_reference_points = key_points.xy[0][filter]\n",
        "frame_reference_key_points = sv.KeyPoints(\n",
        "    xy=frame_reference_points[np.newaxis, ...])\n",
        "\n",
        "pitch_reference_points = np.array(CONFIG.vertices)[filter]\n",
        "\n",
        "transformer = ViewTransformer(\n",
        "    source=pitch_reference_points,\n",
        "    target=frame_reference_points\n",
        ")\n",
        "\n",
        "pitch_all_points = np.array(CONFIG.vertices)\n",
        "frame_all_points = transformer.transform_points(points=pitch_all_points)\n",
        "\n",
        "frame_all_key_points = sv.KeyPoints(xy=frame_all_points[np.newaxis, ...])\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = edge_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    key_points=frame_all_key_points)\n",
        "annotated_frame = vertex_annotator_2.annotate(\n",
        "    scene=annotated_frame,\n",
        "    key_points=frame_all_key_points)\n",
        "annotated_frame = vertex_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    key_points=frame_reference_key_points)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oMXo5k3HkzD"
      },
      "source": [
        "## project ball, players and referies on pitch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9nRbJnksPyE"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "from tqdm import tqdm\n",
        "from sports.common.team import TeamClassifier\n",
        "\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/TestTesi/Ama_test1.mp4\"\n",
        "PLAYER_ID = 2\n",
        "STRIDE = 30\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(\n",
        "    source_path=SOURCE_VIDEO_PATH, stride=STRIDE)\n",
        "\n",
        "crops = []\n",
        "for frame in tqdm(frame_generator, desc='collecting crops'):\n",
        "    result = PLAYER_DETECTION_MODEL.predict(frame, conf=0.3)[0]\n",
        "    detections = sv.Detections.from_ultralytics(result)\n",
        "    players_detections = detections[detections.class_id == PLAYER_ID]\n",
        "    players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]\n",
        "    crops += players_crops\n",
        "\n",
        "team_classifier = TeamClassifier(device=\"cuda\")\n",
        "team_classifier.fit(crops)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0s4SjLIJUwM"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from typing import Optional\n",
        "\n",
        "def draw_pitch_voronoi_diagram_2(\n",
        "    config: SoccerPitchConfiguration,\n",
        "    team_1_xy: np.ndarray,\n",
        "    team_2_xy: np.ndarray,\n",
        "    team_1_color: sv.Color = sv.Color.RED,\n",
        "    team_2_color: sv.Color = sv.Color.WHITE,\n",
        "    opacity: float = 0.5,\n",
        "    padding: int = 50,\n",
        "    scale: float = 0.1,\n",
        "    pitch: Optional[np.ndarray] = None\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Draws a Voronoi diagram on a soccer pitch representing the control areas of two\n",
        "    teams with smooth color transitions.\n",
        "\n",
        "    Args:\n",
        "        config (SoccerPitchConfiguration): Configuration object containing the\n",
        "            dimensions and layout of the pitch.\n",
        "        team_1_xy (np.ndarray): Array of (x, y) coordinates representing the positions\n",
        "            of players in team 1.\n",
        "        team_2_xy (np.ndarray): Array of (x, y) coordinates representing the positions\n",
        "            of players in team 2.\n",
        "        team_1_color (sv.Color, optional): Color representing the control area of\n",
        "            team 1. Defaults to sv.Color.RED.\n",
        "        team_2_color (sv.Color, optional): Color representing the control area of\n",
        "            team 2. Defaults to sv.Color.WHITE.\n",
        "        opacity (float, optional): Opacity of the Voronoi diagram overlay.\n",
        "            Defaults to 0.5.\n",
        "        padding (int, optional): Padding around the pitch in pixels.\n",
        "            Defaults to 50.\n",
        "        scale (float, optional): Scaling factor for the pitch dimensions.\n",
        "            Defaults to 0.1.\n",
        "        pitch (Optional[np.ndarray], optional): Existing pitch image to draw the\n",
        "            Voronoi diagram on. If None, a new pitch will be created. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Image of the soccer pitch with the Voronoi diagram overlay.\n",
        "    \"\"\"\n",
        "    if pitch is None:\n",
        "        pitch = draw_pitch(\n",
        "            config=config,\n",
        "            padding=padding,\n",
        "            scale=scale\n",
        "        )\n",
        "\n",
        "    scaled_width = int(config.width * scale)\n",
        "    scaled_length = int(config.length * scale)\n",
        "\n",
        "    voronoi = np.zeros_like(pitch, dtype=np.uint8)\n",
        "\n",
        "    team_1_color_bgr = np.array(team_1_color.as_bgr(), dtype=np.uint8)\n",
        "    team_2_color_bgr = np.array(team_2_color.as_bgr(), dtype=np.uint8)\n",
        "\n",
        "    y_coordinates, x_coordinates = np.indices((\n",
        "        scaled_width + 2 * padding,\n",
        "        scaled_length + 2 * padding\n",
        "    ))\n",
        "\n",
        "    y_coordinates -= padding\n",
        "    x_coordinates -= padding\n",
        "\n",
        "    def calculate_distances(xy, x_coordinates, y_coordinates):\n",
        "        return np.sqrt((xy[:, 0][:, None, None] * scale - x_coordinates) ** 2 +\n",
        "                       (xy[:, 1][:, None, None] * scale - y_coordinates) ** 2)\n",
        "\n",
        "    distances_team_1 = calculate_distances(team_1_xy, x_coordinates, y_coordinates)\n",
        "    distances_team_2 = calculate_distances(team_2_xy, x_coordinates, y_coordinates)\n",
        "\n",
        "    min_distances_team_1 = np.min(distances_team_1, axis=0)\n",
        "    min_distances_team_2 = np.min(distances_team_2, axis=0)\n",
        "\n",
        "    # Increase steepness of the blend effect\n",
        "    steepness = 15  # Increased steepness for sharper transition\n",
        "    distance_ratio = min_distances_team_2 / np.clip(min_distances_team_1 + min_distances_team_2, a_min=1e-5, a_max=None)\n",
        "    blend_factor = np.tanh((distance_ratio - 0.5) * steepness) * 0.5 + 0.5\n",
        "\n",
        "    # Create the smooth color transition\n",
        "    for c in range(3):  # Iterate over the B, G, R channels\n",
        "        voronoi[:, :, c] = (blend_factor * team_1_color_bgr[c] +\n",
        "                            (1 - blend_factor) * team_2_color_bgr[c]).astype(np.uint8)\n",
        "\n",
        "    overlay = cv2.addWeighted(voronoi, opacity, pitch, 1 - opacity, 0)\n",
        "\n",
        "    return overlay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rjfNfzErrSN"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "from sports.annotators.soccer import (\n",
        "    draw_pitch,\n",
        "    draw_points_on_pitch,\n",
        "    draw_pitch_voronoi_diagram\n",
        ")\n",
        "\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/TestTesi/Ama_test1.mp4\"\n",
        "BALL_ID = 0\n",
        "GOALKEEPER_ID = 1\n",
        "PLAYER_ID = 2\n",
        "REFEREE_ID = 3\n",
        "\n",
        "ellipse_annotator = sv.EllipseAnnotator(\n",
        "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
        "    thickness=2\n",
        ")\n",
        "label_annotator = sv.LabelAnnotator(\n",
        "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
        "    text_color=sv.Color.from_hex('#000000'),\n",
        "    text_position=sv.Position.BOTTOM_CENTER\n",
        ")\n",
        "triangle_annotator = sv.TriangleAnnotator(\n",
        "    color=sv.Color.from_hex('#FFD700'),\n",
        "    base=20, height=17\n",
        ")\n",
        "\n",
        "tracker = sv.ByteTrack()\n",
        "tracker.reset()\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame = next(frame_generator)\n",
        "\n",
        "# ball, goalkeeper, player, referee detection\n",
        "\n",
        "result = PLAYER_DETECTION_MODEL.predict(frame, conf=0.3)[0]\n",
        "detections = sv.Detections.from_ultralytics(result)\n",
        "\n",
        "ball_detections = detections[detections.class_id == BALL_ID]\n",
        "ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
        "\n",
        "all_detections = detections[detections.class_id != BALL_ID]\n",
        "all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)\n",
        "all_detections = tracker.update_with_detections(detections=all_detections)\n",
        "\n",
        "goalkeepers_detections = all_detections[all_detections.class_id == GOALKEEPER_ID]\n",
        "players_detections = all_detections[all_detections.class_id == PLAYER_ID]\n",
        "referees_detections = all_detections[all_detections.class_id == REFEREE_ID]\n",
        "\n",
        "# team assignment\n",
        "\n",
        "players_crops = [sv.crop_image(frame, xyxy) for xyxy in players_detections.xyxy]\n",
        "players_detections.class_id = team_classifier.predict(players_crops)\n",
        "\n",
        "goalkeepers_detections.class_id = resolve_goalkeepers_team_id(\n",
        "    players_detections, goalkeepers_detections)\n",
        "\n",
        "referees_detections.class_id -= 1\n",
        "\n",
        "all_detections = sv.Detections.merge([\n",
        "    players_detections, goalkeepers_detections, referees_detections])\n",
        "\n",
        "# frame visualization\n",
        "\n",
        "labels = [\n",
        "    f\"#{tracker_id}\"\n",
        "    for tracker_id\n",
        "    in all_detections.tracker_id\n",
        "]\n",
        "\n",
        "all_detections.class_id = all_detections.class_id.astype(int)\n",
        "\n",
        "annotated_frame = frame.copy()\n",
        "annotated_frame = ellipse_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=all_detections)\n",
        "annotated_frame = label_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=all_detections,\n",
        "    labels=labels)\n",
        "annotated_frame = triangle_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=ball_detections)\n",
        "\n",
        "sv.plot_image(annotated_frame)\n",
        "\n",
        "players_detections = sv.Detections.merge([\n",
        "    players_detections, goalkeepers_detections\n",
        "])\n",
        "\n",
        "# detect pitch key points\n",
        "\n",
        "result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "key_points = sv.KeyPoints.from_inference(result)\n",
        "\n",
        "# project ball, players and referies on pitch\n",
        "\n",
        "filter = key_points.confidence[0] > 0.5\n",
        "frame_reference_points = key_points.xy[0][filter]\n",
        "pitch_reference_points = np.array(CONFIG.vertices)[filter]\n",
        "\n",
        "transformer = ViewTransformer(\n",
        "    source=frame_reference_points,\n",
        "    target=pitch_reference_points\n",
        ")\n",
        "\n",
        "frame_ball_xy = ball_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
        "pitch_ball_xy = transformer.transform_points(points=frame_ball_xy)\n",
        "\n",
        "players_xy = players_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
        "pitch_players_xy = transformer.transform_points(points=players_xy)\n",
        "\n",
        "referees_xy = referees_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
        "pitch_referees_xy = transformer.transform_points(points=referees_xy)\n",
        "\n",
        "# visualize video game-style radar view\n",
        "\n",
        "annotated_frame = draw_pitch(CONFIG)\n",
        "annotated_frame = draw_points_on_pitch(\n",
        "    config=CONFIG,\n",
        "    xy=pitch_ball_xy,\n",
        "    face_color=sv.Color.WHITE,\n",
        "    edge_color=sv.Color.BLACK,\n",
        "    radius=10,\n",
        "    pitch=annotated_frame)\n",
        "annotated_frame = draw_points_on_pitch(\n",
        "    config=CONFIG,\n",
        "    xy=pitch_players_xy[players_detections.class_id == 0],\n",
        "    face_color=sv.Color.from_hex('00BFFF'),\n",
        "    edge_color=sv.Color.BLACK,\n",
        "    radius=16,\n",
        "    pitch=annotated_frame)\n",
        "annotated_frame = draw_points_on_pitch(\n",
        "    config=CONFIG,\n",
        "    xy=pitch_players_xy[players_detections.class_id == 1],\n",
        "    face_color=sv.Color.from_hex('FF1493'),\n",
        "    edge_color=sv.Color.BLACK,\n",
        "    radius=16,\n",
        "    pitch=annotated_frame)\n",
        "annotated_frame = draw_points_on_pitch(\n",
        "    config=CONFIG,\n",
        "    xy=pitch_referees_xy,\n",
        "    face_color=sv.Color.from_hex('FFD700'),\n",
        "    edge_color=sv.Color.BLACK,\n",
        "    radius=16,\n",
        "    pitch=annotated_frame)\n",
        "\n",
        "sv.plot_image(annotated_frame)\n",
        "\n",
        "# visualize voronoi diagram\n",
        "\n",
        "annotated_frame = draw_pitch(CONFIG)\n",
        "annotated_frame = draw_pitch_voronoi_diagram(\n",
        "    config=CONFIG,\n",
        "    team_1_xy=pitch_players_xy[players_detections.class_id == 0],\n",
        "    team_2_xy=pitch_players_xy[players_detections.class_id == 1],\n",
        "    team_1_color=sv.Color.from_hex('00BFFF'),\n",
        "    team_2_color=sv.Color.from_hex('FF1493'),\n",
        "    pitch=annotated_frame)\n",
        "\n",
        "sv.plot_image(annotated_frame)\n",
        "\n",
        "# visualize voronoi diagram with blend\n",
        "\n",
        "annotated_frame = draw_pitch(\n",
        "    config=CONFIG,\n",
        "    background_color=sv.Color.WHITE,\n",
        "    line_color=sv.Color.BLACK\n",
        ")\n",
        "annotated_frame = draw_pitch_voronoi_diagram_2(\n",
        "    config=CONFIG,\n",
        "    team_1_xy=pitch_players_xy[players_detections.class_id == 0],\n",
        "    team_2_xy=pitch_players_xy[players_detections.class_id == 1],\n",
        "    team_1_color=sv.Color.from_hex('00BFFF'),\n",
        "    team_2_color=sv.Color.from_hex('FF1493'),\n",
        "    pitch=annotated_frame)\n",
        "annotated_frame = draw_points_on_pitch(\n",
        "    config=CONFIG,\n",
        "    xy=pitch_ball_xy,\n",
        "    face_color=sv.Color.WHITE,\n",
        "    edge_color=sv.Color.WHITE,\n",
        "    radius=8,\n",
        "    thickness=1,\n",
        "    pitch=annotated_frame)\n",
        "annotated_frame = draw_points_on_pitch(\n",
        "    config=CONFIG,\n",
        "    xy=pitch_players_xy[players_detections.class_id == 0],\n",
        "    face_color=sv.Color.from_hex('00BFFF'),\n",
        "    edge_color=sv.Color.WHITE,\n",
        "    radius=16,\n",
        "    thickness=1,\n",
        "    pitch=annotated_frame)\n",
        "annotated_frame = draw_points_on_pitch(\n",
        "    config=CONFIG,\n",
        "    xy=pitch_players_xy[players_detections.class_id == 1],\n",
        "    face_color=sv.Color.from_hex('FF1493'),\n",
        "    edge_color=sv.Color.WHITE,\n",
        "    radius=16,\n",
        "    thickness=1,\n",
        "    pitch=annotated_frame)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ7CP3RFoX4R"
      },
      "source": [
        "## ball tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFMa2ER_JUtG"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "import supervision as sv\n",
        "from sports.annotators.soccer import draw_pitch, draw_points_on_pitch\n",
        "\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/TestTesi/Ama_test1.mp4\"\n",
        "BALL_ID = 0\n",
        "MAXLEN = 5\n",
        "\n",
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "\n",
        "path_raw = []\n",
        "M = deque(maxlen=MAXLEN)\n",
        "\n",
        "for frame in tqdm(frame_generator, total=video_info.total_frames):\n",
        "\n",
        "    result = PLAYER_DETECTION_MODEL.predict(frame, conf=0.05)[0]\n",
        "    detections = sv.Detections.from_ultralytics(result)\n",
        "\n",
        "    ball_detections = detections[detections.class_id == BALL_ID]\n",
        "    ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
        "\n",
        "    result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.05)[0]\n",
        "    key_points = sv.KeyPoints.from_inference(result)\n",
        "\n",
        "    filter = key_points.confidence[0] > 0.15\n",
        "    frame_reference_points = key_points.xy[0][filter]\n",
        "    pitch_reference_points = np.array(CONFIG.vertices)[filter]\n",
        "\n",
        "    transformer = ViewTransformer(\n",
        "        source=frame_reference_points,\n",
        "        target=pitch_reference_points\n",
        "    )\n",
        "    M.append(transformer.m)\n",
        "    transformer.m = np.mean(np.array(M), axis=0)\n",
        "\n",
        "    frame_ball_xy = ball_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
        "    pitch_ball_xy = transformer.transform_points(points=frame_ball_xy)\n",
        "\n",
        "    path_raw.append(pitch_ball_xy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAD53KnFp8dw"
      },
      "outputs": [],
      "source": [
        "path = [\n",
        "    np.empty((0, 2), dtype=np.float32) if coorinates.shape[0] >= 2 else coorinates\n",
        "    for coorinates\n",
        "    in path_raw\n",
        "]\n",
        "\n",
        "path = [coorinates.flatten() for coorinates in path]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwiq0zIAq2JE"
      },
      "outputs": [],
      "source": [
        "from sports.annotators.soccer import draw_paths_on_pitch\n",
        "\n",
        "annotated_frame = draw_pitch(CONFIG)\n",
        "annotated_frame = draw_paths_on_pitch(\n",
        "    config=CONFIG,\n",
        "    paths=[path],\n",
        "    color=sv.Color.WHITE,\n",
        "    pitch=annotated_frame)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWksgzo4q5cr"
      },
      "outputs": [],
      "source": [
        "from typing import List, Union\n",
        "\n",
        "def replace_outliers_based_on_distance(\n",
        "    positions: List[np.ndarray],\n",
        "    distance_threshold: float\n",
        ") -> List[np.ndarray]:\n",
        "    last_valid_position: Union[np.ndarray, None] = None\n",
        "    cleaned_positions: List[np.ndarray] = []\n",
        "\n",
        "    for position in positions:\n",
        "        if len(position) == 0:\n",
        "            # If the current position is already empty, just add it to the cleaned positions\n",
        "            cleaned_positions.append(position)\n",
        "        else:\n",
        "            if last_valid_position is None:\n",
        "                # If there's no valid last position, accept the first valid one\n",
        "                cleaned_positions.append(position)\n",
        "                last_valid_position = position\n",
        "            else:\n",
        "                # Calculate the distance from the last valid position\n",
        "                distance = np.linalg.norm(position - last_valid_position)\n",
        "                if distance > distance_threshold:\n",
        "                    # Replace with empty array if the distance exceeds the threshold\n",
        "                    cleaned_positions.append(np.array([], dtype=np.float64))\n",
        "                else:\n",
        "                    cleaned_positions.append(position)\n",
        "                    last_valid_position = position\n",
        "\n",
        "    return cleaned_positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0wmKU-qq7zP"
      },
      "outputs": [],
      "source": [
        "MAX_DISTANCE_THRESHOLD = 500\n",
        "\n",
        "path = replace_outliers_based_on_distance(path, MAX_DISTANCE_THRESHOLD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXL4eqOTrASQ"
      },
      "outputs": [],
      "source": [
        "from sports.annotators.soccer import draw_paths_on_pitch\n",
        "\n",
        "annotated_frame = draw_pitch(CONFIG)\n",
        "annotated_frame = draw_paths_on_pitch(\n",
        "    config=CONFIG,\n",
        "    paths=[path],\n",
        "    color=sv.Color.WHITE,\n",
        "    pitch=annotated_frame)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}